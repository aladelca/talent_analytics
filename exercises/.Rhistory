metrics = c('accuracy'))
x_train_esc = predict(esc, newdata = x_train)
x_test_esc = predict(esc, newdata = x_test)
x_train_esc = as.matrix(x_train_esc)
y_train_mat <- to_categorical(train$Survived, num_classes =  2)
modelo_nn %>% fit(x_train_esc, y_train_mat, epochs = 100)
modelo_nn %>% compile(loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy'))
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(9)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_esc, y_train_mat, epochs = 100)
library(readr)
library(tidyverse)
data = read_csv('https://raw.githubusercontent.com/aladelca/machine_learning_model/main/archivos_trabajo/titanic.csv')
split = sample(c(rep(0,0.7*nrow(data)),rep(1,0.3*nrow(data))))
split[891] = 1
data$split = split
train = data[split == 0,]
test = data[split==1,]
train = train %>% select(Pclass,Sex,Age,SibSp,Parch,Fare,Embarked,Survived)
test = test %>% select(Pclass,Sex,Age,SibSp,Parch,Fare,Embarked,Survived)
library(caret)
dummy_train = dummyVars('~.', data = train)
train_final = data.frame(predict(dummy_train, newdata = train))
test_final = data.frame(predict(dummy_train, newdat = test))
y_train = train_final$Survived
y_test = test_final$Survived
x_train = train_final %>% select(Pclass, Sexfemale, Sexmale, Age, SibSp, Parch, EmbarkedC, EmbarkedQ, EmbarkedS)
x_test = test_final %>% select(Pclass, Sexfemale, Sexmale, Age, SibSp, Parch, EmbarkedC, EmbarkedQ, EmbarkedS)
nentradas = ncol(x_train)
nentradas
## Instalar librerÃ­as
install.packages('keras')
install.packages('tensorflow')
install.packages("keras")
library(keras)
library(tensorflow)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(9)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
esc = preProcess(x_train, method = 'scale')
x_train_esc = predict(esc, newdata = x_train)
x_test_esc = predict(esc, newdata = x_test)
x_train_esc = as.matrix(x_train_esc)
y_train_mat <- to_categorical(train$Survived, num_classes =  2)
modelo_nn %>% fit(x_train_esc, y_train_mat, epochs = 100)
x_train
x_train_no_dummies = %>% select(Pclass, Age, SibSp, Parch, Fare)
x_train_no_dummies = train %>% select(Pclass, Age, SibSp, Parch, Fare)
x_test_no_dummies = test %>% select(Pclass, Age, SibSp, Parch, Fare)
nentradas = ncol(x_train, method = 'scale')
esc = preProcess(x_train_no_dummies, method = 'scale')
x_train_no_dummies_esc = predict(esc, newdata = x_train_no_dummies)
x_test_no_dummies_esc = predict(esc, newdata = x_test_no_dummies)
x_train_no_dummies_esc = as.matrix(x_train_no_dummies_esc)
x_test_no_dummies_esc = as.matrix(x_test_no_dummies_esc)
train
y_train = to_categorical(train$Survived, num_classes = 2)
y_train
y_test = to_categorical(test$Survived, num_classes = 2)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(9)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
View(x_train_no_dummies_esc)
modelo_nn %>% fit(x_train_no_dummies_esc, y_train, epochs = 20)
modelo_nn %>% evaluate(x_test_no_dummies_esc, y_test)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 2, activation = 'sigmoid')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, train$Survived, epochs = 20)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 2, activation = 'sigmoid')
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, train$Survived, epochs = 20)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, train$Survived, epochs = 20)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, as.factor(train$Survived), epochs = 20)
as.factor(train$Survived)
as.matrix(train$Survived)
modelo_nn %>% fit(x_train_no_dummies_esc, as.matrix(train$Survived), epochs = 20)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 120, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'sigmoid') %>%
layer_dense(units = 64, activation = 'sigmoid') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, as.matrix(train$Survived), epochs = 20)
modelo_nn %>% evaluate(x_test_no_dummies_esc, y_test)
modelo_nn %>% evaluate(x_test_no_dummies_esc, as.matrix(test$Survived))
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 32, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'sigmoid') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, as.matrix(train$Survived), epochs = 20)
modelo_nn %>% fit(x_train_no_dummies_esc, as.matrix(train$Survived), epochs = 100)
keras::k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 32, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dense(units = 1, activation = 'sigmoid')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, as.matrix(train$Survived), epochs = 100)
k_clear_session()
modelo_nn = keras_model_sequential()
modelo_nn %>%
layer_dense(units = 32, activation = 'relu', input_shape = c(5)) %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dense(units = 64, activation = 'tanh') %>%
layer_dense(units = 1, activation = 'sigmoid')
summary(modelo_nn)
modelo_nn %>% compile(loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy'))
modelo_nn %>% fit(x_train_no_dummies_esc, as.matrix(train$Survived), epochs = 100)
400*400*255
mnist = dataset_mnist()
x_train = mnist$train$x
dim(x_train)
60000*28*28
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
28*28
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
dim(x_train)
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_train)[1],784))
x_train = x_train/255
x_test = x_test/255
mnist = dataset_mnist()
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_test)[1],784))
mnist = dataset_mnist()
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_test)[1],784))
x_train = x_train/255
x_test = x_test/255
k_clear_session()
y_test
modelo_redes %>%
layer_dense(128, activation = 'relu', input_shape = c(784)) %>%
layer_dense(64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(10, activation = 'softmax')
modelo_redes = keras_model_sequential()
modelo_redes %>%
layer_dense(128, activation = 'relu', input_shape = c(784)) %>%
layer_dense(64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(10, activation = 'softmax')
modelo_redes %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history = modelo_redes %>% fit(
x_train, y_train,
validation_data = list(x_test, y_test),
epochs = 20
)
y_train
y_train
as_categorical(y_train,num_classes = 10)
to_categorical(y_train,num_classes = 10)
history = modelo_redes %>% fit(
x_train, to_categorical(y_train,num_classes = 10),
validation_data = list(x_test, to_categorical(y_test,num_classes = 10)),
epochs = 20
)
as_matrix(y_train)
as.matrix(y_train)
history = modelo_redes %>% fit(
x_train, as.matrix(y_train),
validation_data = list(x_test, as.matrix(y_test)),
epochs = 20
)
mnist = dataset_mnist()
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_test)[1],784))
x_train = x_train/255
x_test = x_test/255
k_clear_session()
modelo_redes = keras_model_sequential()
modelo_redes %>%
layer_dense(128, activation = 'relu', input_shape = c(784)) %>%
layer_dense(64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(10, activation = 'softmax')
modelo_redes %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history = modelo_redes %>% fit(
x_train, as.matrix(y_train),
validation_data = list(x_test, as.matrix(y_test)),
epochs = 20
)
mnist = dataset_mnist()
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_test)[1],784))
x_train = x_train/255
x_test = x_test/255
k_clear_session()
modelo_redes = keras_model_sequential()
modelo_redes %>%
layer_dense(128, activation = 'relu', input_shape = c(784)) %>%
layer_dense(64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(1, activation = 'sigmoid')
modelo_redes %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history = modelo_redes %>% fit(
x_train, as.matrix(y_train),
validation_data = list(x_test, as.matrix(y_test)),
epochs = 20
)
mnist = dataset_mnist()
x_train = mnist$train$x
y_train = mnist$train$y
x_test =mnist$test$x
y_test = mnist$test$y
### Reshape de datos
x_train = array_reshape(x_train, c(dim(x_train)[1],784))
x_test = array_reshape(x_test, c(dim(x_test)[1],784))
x_train = x_train/255
x_test = x_test/255
y_train <- to_categorical(y_train, num_classes = 10)
y_test <- to_categorical(y_test, num_classes = 10)
k_clear_session()
modelo_redes = keras_model_sequential()
modelo_redes %>%
layer_dense(128, activation = 'relu', input_shape = c(784)) %>%
layer_dense(64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(10, activation = 'softmax')
modelo_redes %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
history = modelo_redes %>% fit(
x_train, y_train,
validation_data = list(x_test, y_test),
epochs = 20
)
predict(modelo_redes, x_test)
preds = predict(modelo_redes, x_test)
preds[1,]
max(preds[1,])
which.max(preds[1,])
which.max(preds[1,])-1
mnist
mnist$test
mnist$test$x
mnist$test$x[1]
mnist$test$x[1,]
mnist$test$x[1]
x_train[1,]
imagen = matrix(x_train[1,], nrow = 28, ncol = 28)
image(imagen)
which.max(preds[1,])-1
which.max(preds[2,])-1
imagen = matrix(x_train[2,], nrow = 28, ncol = 28)
image(imagen)
which.max(preds[3,])-1
imagen = matrix(x_train[2,], nrow = 28, ncol = 28)
which.max(preds[3,])-1
imagen = matrix(x_test[3,], nrow = 28, ncol = 28)
image(imagen)
which.max(preds[4,])-1
imagen = matrix(x_test[4,], nrow = 28, ncol = 28)
image(imagen)
indice = 100
which.max(preds[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
indice = 50
which.max(preds[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
indice = 400
which.max(preds[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
indice = 20
which.max(preds[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
indice = 250
which.max(preds[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
y_test
y_test[indice,]
wich.max(y_test[indice,])
which.max(y_test[indice,])
which.max(y_test[indice,])-1
indice = 250
which.max(preds[indice,])-1
which.max(y_test[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
indice = 400
which.max(preds[indice,])-1
which.max(y_test[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
indice = 1000
which.max(preds[indice,])-1
which.max(y_test[indice,])-1
imagen = matrix(x_test[indice,], nrow = 28, ncol = 28)
image(imagen)
imagen
which.max(preds[indice,])-1
which.max(y_test[indice,])-1
test = read.csv('/Users/aladelca/Downloads/house-prices-advanced-regression-techniques-g2/test (4).csv')
train = read.csv('/Users/aladelca/Downloads/house-prices-advanced-regression-techniques-g2/train (3).csv')
train
library(tidyverse)
train %>% select(LotFrontage, LotArea,MasVnrArea)
train %>% select(LotFrontage, LotArea,MasVnrArea, WoodDeckSF,SalePrice)
y_train = train %>% select(SalePrice)
x_train = train %>% select(LotFrontage, LotArea,MasVnrArea, WoodDeckSF)
train_percentage <- 0.7
set.seed(123)
library(caret)
train_indices <- createDataPartition(x_train, p = train_percentage, list = FALSE)
x_train = train %>% select(Id,LotFrontage, LotArea,MasVnrArea, WoodDeckSF)
train_indices <- createDataPartition(x_train$Id, p = train_percentage, list = FALSE)
View(train)
x_training <- x_train[train_indices, ]
x_validation <- x_train[-train_indices, ]
y_training <- y_train[train_indices, ]
y_validation <- y_train[-train_indices, ]
x_training
x_training, y_training
x_training,
y_training
library(lightgbm)
train_matrix = data.matrix(x_training)
test_matrix = data.matrix(x_validation)
lgb_train = lgb.Dataset(data = train_matrix,label = y_training)
modelo = lgb.train(data = lgb_train)
params <- list(
objective = "regression",
metric = "mse",
boosting_type = "gbdt",
num_leaves = 31,
learning_rate = 0.05,
nrounds = 100
)
modelo = lgb.train(params = params ,data = lgb_train)
predict(test_matrix)
test_matrix = data.matrix(x_validation)
predict(modelo,test_matrix)
y_preds = predict(modelo,test_matrix)
y_validation
(y_preds - y_validation)
(y_preds - y_validation)**@
(y_preds - y_validation)**2
mean((y_preds - y_validation)**2)
(mean((y_preds - y_validation)**2))**(1/2)
val_matrix = data.matrix(x_validation)
y_preds = predict(modelo,val_matrix)
test = read.csv('/Users/aladelca/Downloads/house-prices-advanced-regression-techniques-g2/test (4).csv')
test_train = test %>% select(Id,LotFrontage, LotArea,MasVnrArea, WoodDeckSF)
test_matrix = data.matrix(test_train)
predict(modelo, test_matrix)
as.dataframe(predict(modelo, test_matrix))
data.frame(predict(modelo, test_matrix))
respuestas = data.frame(predict(modelo, test_matrix))
respuestas
names(respuestas[1]) = 'SalePrice'
respuesta
respuestas
names(respuestas)[1] = 'SalePrice'
respuestas$Id = test$Id
write_csv(respuestas, file = '/Users/aladelca/Downloads/respuestas_finales.csv')
factorial(10)
(factorial(45) / (factorial(30) * factorial(15)))*(factorial(255)/(factorial(170)*factorial(85))) * (factorial(200)* factorial(100))/(factorial(300)* factorial(200)*factorial(100))
library('summarytools')
setwd("~/OneDrive - McGill University/MMA/Talent_analytics")
setwd("~/OneDrive - McGill University/MMA/Talent_analytics/talent_analytics/exercises")
library('tidyverse')
library('ggplot')
library('ggplot2')
mtcars
ggplot(data = mtcars, aes (x = hp, y = drat)) + geom_point()
knit_with_parameters("~/Library/CloudStorage/OneDrive-McGillUniversity/MMA/Talent_analytics/talent_analytics/exercises/hello-ta.Rmd")
install.packages("tinytex")
